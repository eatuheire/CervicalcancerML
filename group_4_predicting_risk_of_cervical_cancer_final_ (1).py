# -*- coding: utf-8 -*-
"""Group 4 PREDICTING RISK OF CERVICAL CANCER final .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nkn8KN4J1t2YnC8vTToDvzM5Ub19VeP0

# **PREDICTING RISK OF CERVICAL CANCER USING MACHINE LEARNING**

BY Group 4 ; CLARE , ELIZABETH , LYDIA

LYDIA: Data loading, exploration, and initial preprocessing

CLARE: KNN imputation, EDA visualizations, feature importance

ELIZABETH: Model implementation, evaluation, Streamlit app

ALL: Analysis, interpretation, and presentation preparation
"""

# all necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import KNNImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve
import warnings
warnings.filterwarnings('ignore')

# visualizations
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Load and explore the dataset
from google.colab import drive
drive.mount('/content/gdrive')
df = pd.read_csv('/content/gdrive/MyDrive/risk_factors_cervical_cancer.csv')
print("Dataset Shape:", df.shape)
print("\nFirst few rows:")
print(df.head())

print("\nDataset Info:")
print(df.info())

print("\nMissing values summary:")
# Count missing values (represented as '?')
missing_count = (df == '?').sum()
print(missing_count[missing_count > 0])

print("\nTarget variable distribution (Biopsy):")
print(df['Biopsy'].value_counts())

# Data Preprocessing and Cleaning

# Replace '?' with NaN for proper handling
df.replace('?', np.nan, inplace=True)

# Convert all columns to numeric where possible
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

print("Data types after conversion:")
print(df.dtypes.value_counts())

# Analyze missing values after conversion
print("\nMissing values percentage:")
missing_percentage = (df.isnull().sum() / len(df)) * 100
missing_percentage = missing_percentage[missing_percentage > 0].sort_values(ascending=False)
print(missing_percentage)

# Drop columns with too many missing values (>50%)
columns_to_drop = missing_percentage[missing_percentage > 50].index
print(f"\nDropping columns with >50% missing values: {list(columns_to_drop)}")
df_clean = df.drop(columns=columns_to_drop)

print(f"Shape after dropping high-missing columns: {df_clean.shape}")

# Handle remaining missing values using KNN Imputation done by clare

X = df_clean.drop('Biopsy', axis=1)
y = df_clean['Biopsy']

print("Before imputation - Missing values in features:")
print(X.isnull().sum().sum())

# KNN Imputer
imputer = KNNImputer(n_neighbors=5)
X_imputed = imputer.fit_transform(X)
X_imputed = pd.DataFrame(X_imputed, columns=X.columns)

print("After imputation - Missing values in features:")
print(X_imputed.isnull().sum().sum())

print(f"Final dataset shape: {X_imputed.shape}")
print(f"Target distribution: {y.value_counts()}")
print(f"Class ratio: {y.value_counts(normalize=True)}")

# summary dataframe for analysis
analysis_df = X_imputed.copy()
analysis_df['Biopsy'] = y

# 1. Target distribution
plt.figure(figsize=(15, 12))

plt.subplot(2, 3, 1)
y.value_counts().plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('Distribution of Biopsy Results')
plt.xlabel('Biopsy Result (0=Negative, 1=Positive)')
plt.ylabel('Count')
for i, v in enumerate(y.value_counts()):
    plt.text(i, v + 5, str(v), ha='center', va='bottom')

# 2. Age distribution by biopsy result ()
plt.subplot(2, 3, 2)
sns.boxplot(x='Biopsy', y='Age', data=analysis_df, palette=['skyblue', 'salmon'])
plt.title('Age Distribution by Biopsy Result')

# 3. Number of sexual partners by biopsy result
plt.subplot(2, 3, 3)
sns.boxplot(x='Biopsy', y='Number of sexual partners', data=analysis_df, palette=['skyblue', 'salmon'])
plt.title('Number of Sexual Partners by Biopsy Result')

# 4. Correlation heatmap
plt.subplot(2, 3, 4)
# Select top features for correlation
top_features = ['Age', 'Number of sexual partners', 'First sexual intercourse',
               'Num of pregnancies', 'Smokes', 'Hormonal Contraceptives',
               'STDs', 'STDs (number)', 'Biopsy']
corr_matrix = analysis_df[top_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap of Key Features')

# 5. STDs presence by biopsy result
plt.subplot(2, 3, 5)
std_biopsy = pd.crosstab(analysis_df['STDs'], analysis_df['Biopsy'])
std_biopsy.plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('STDs Presence vs Biopsy Result')
plt.xlabel('STDs (0=No, 1=Yes)')

plt.tight_layout()
plt.show()

# 6. preview using Random Forest
plt.figure(figsize=(10, 8))
rf_preview = RandomForestClassifier(n_estimators=100, random_state=42)
rf_preview.fit(X_imputed, y)
feature_importance = pd.DataFrame({
    'feature': X_imputed.columns,
    'importance': rf_preview.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(data=feature_importance.head(15), x='importance', y='feature')
plt.title('Top 15 Most Important Features (Random Forest)')
plt.xlabel('Feature Importance')
plt.tight_layout()
plt.show()

#univariate
#histogram showing age distribution
ax=df['Age'].hist(bins='auto' ,edgecolor = 'black')
ax.ticklabel_format(style='plain');
ax.set_title('Distribution of Age')
ax.set_xlabel('Age (Years)')
ax.set_ylabel('Frequency')

# Multi variate analysis
# GRouped bar chart
 #Count value frequencies for each column
cols = ["Schiller", "Citology", "Biopsy"]
counts = pd.DataFrame()
for c in cols:
    counts[c] = df[c].value_counts()
# Transpose so columns become groups
counts = counts.T
# Plot grouped bar chart
counts.plot(kind="bar", figsize=(8,5))
plt.title("Counts of Target Variables")
plt.xlabel("Target Variable")
plt.ylabel("Count")
plt.legend(title="Value (0/1)")
plt.show()

# Prepare data for modeling
# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")
print(f"Training set class distribution:\n{y_train.value_counts()}")
print(f"Test set class distribution:\n{y_test.value_counts()}")

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert to DataFrames for better handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_imputed.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_imputed.columns)

# Model Training and Evaluation
# Initialize models
models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42, probability=True),
    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')
}


results = {}

# Train and evaluate each model
for name, model in models.items():
    print(f"\n{'='*50}")
    print(f"Training {name}...")

    # Train model
    if name in ['Random Forest', 'SVM']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_pred_proba)

    # Store results
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc_roc': auc_roc,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }

    print(f"{name} Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"AUC-ROC: {auc_roc:.4f}")

    # Classification report
    print(f"\nClassification Report for {name}:")
    print(classification_report(y_test, y_pred))

#neural network
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
import warnings
warnings.filterwarnings('ignore')

print("\n==================================================")
print("Training MLPClassifier (Neural Network)...")

# Initialize MLPClassifier

mlp_model = MLPClassifier(
    hidden_layer_sizes=(100, 50), # Two hidden layers with 100 and 50 neurons
    max_iter=1000, # Increased iterations for convergence
    random_state=42,
    activation='relu', # Rectified Linear Unit activation function
    solver='adam', # optimizer
    learning_rate_init=0.001,
    alpha=0.0001, # L2 penalty (regularization term) parameter
    verbose=True # To see training progress
)

# Train the MLP model on the scaled training data
mlp_model.fit(X_train_scaled, y_train)

# Make predictions on the scaled test data
y_pred_mlp = mlp_model.predict(X_test_scaled)
y_pred_proba_mlp = mlp_model.predict_proba(X_test_scaled)[:, 1]

# Calculate metrics
accuracy_mlp = accuracy_score(y_test, y_pred_mlp)
precision_mlp = precision_score(y_test, y_pred_mlp)
recall_mlp = recall_score(y_test, y_pred_mlp)
f1_mlp = f1_score(y_test, y_pred_mlp)
auc_roc_mlp = roc_auc_score(y_test, y_pred_proba_mlp)

# Store results for comparison
results['MLP (Neural Network)'] = {
    'model': mlp_model,
    'accuracy': accuracy_mlp,
    'precision': precision_mlp,
    'recall': recall_mlp,
    'f1': f1_mlp,
    'auc_roc': auc_roc_mlp,
    'predictions': y_pred_mlp,
    'probabilities': y_pred_proba_mlp
}

print("\nMLP (Neural Network) Results:")
print(f"Accuracy: {accuracy_mlp:.4f}")
print(f"Precision: {precision_mlp:.4f}")
print(f"Recall: {recall_mlp:.4f}")
print(f"F1-Score: {f1_mlp:.4f}")
print(f"AUC-ROC: {auc_roc_mlp:.4f}")

# Classification report
print(f"\nClassification Report for MLP (Neural Network):")
print(classification_report(y_test, y_pred_mlp))

# Model Comparison and Visualization

# Comparison DataFrame
results_df = pd.DataFrame({
    name: [
        results[name]['accuracy'],
        results[name]['precision'],
        results[name]['recall'],
        results[name]['f1'],
        results[name]['auc_roc']
    ] for name in results.keys()
}, index=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']).T

print("Model Performance Comparison:")
print(results_df.round(4))

# Plot performance comparison
plt.figure(figsize=(15, 10))

# Metrics comparison
plt.subplot(2, 2, 1)
results_df[['Accuracy', 'Precision', 'Recall', 'F1-Score']].plot(kind='bar', ax=plt.gca())
plt.title('Model Performance Metrics Comparison')
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

# AUC-ROC comparison
plt.subplot(2, 2, 2)
results_df['AUC-ROC'].plot(kind='bar', color='pink', ax=plt.gca())
plt.title('AUC-ROC Score Comparison')
plt.xticks(rotation=45)
plt.ylabel('AUC-ROC Score')

# ROC Curves
plt.subplot(2, 2, 3)
for name in models.keys():
    fpr, tpr, _ = roc_curve(y_test, results[name]['probabilities'])
    plt.plot(fpr, tpr, label=f'{name} (AUC = {results[name]["auc_roc"]:.3f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend()

# Focus on Recall (most important metric for medical diagnosis)
plt.subplot(2, 2, 4)
results_df['Recall'].plot(kind='bar', color='red', ax=plt.gca())
plt.title('Recall Score Comparison (Minimize False Negatives)')
plt.xticks(rotation=45)
plt.ylabel('Recall Score')

plt.tight_layout()
plt.show()

#Cross-Validation for more robust evaluation
print("Performing Cross-Validation...")

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_results = {}

for name, model in models.items():
    print(f"\nCross-validating {name}...")

    if name in ['Logistic Regression', 'SVM']:
        X_cv = X_train_scaled
    else:
        X_cv = X_train

    # Cross-validation scores
    cv_accuracy = cross_val_score(model, X_cv, y_train, cv=cv, scoring='accuracy')
    cv_precision = cross_val_score(model, X_cv, y_train, cv=cv, scoring='precision')
    cv_recall = cross_val_score(model, X_cv, y_train, cv=cv, scoring='recall')
    cv_f1 = cross_val_score(model, X_cv, y_train, cv=cv, scoring='f1')
    cv_auc = cross_val_score(model, X_cv, y_train, cv=cv, scoring='roc_auc')

    cv_results[name] = {
        'CV Accuracy': cv_accuracy.mean(),
        'CV Precision': cv_precision.mean(),
        'CV Recall': cv_recall.mean(),
        'CV F1': cv_f1.mean(),
        'CV AUC': cv_auc.mean()
    }

    print(f"CV Accuracy: {cv_accuracy.mean():.4f} (+/- {cv_accuracy.std() * 2:.4f})")
    print(f"CV Recall: {cv_recall.mean():.4f} (+/- {cv_recall.std() * 2:.4f})")

# CV comparison
cv_df = pd.DataFrame(cv_results).T
print("\nCross-Validation Results:")
print(cv_df.round(4))

import joblib
# Hyperparameter Tuning for the best model

# tune the best performing model
print("Performing Hyperparameter Tuning...")

# Let's assume Random Forest or XGBoost performed best
best_model_name = max(results.keys(), key=lambda x: results[x]['recall'])
print(f"Tuning the best model: {best_model_name}")

if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    model = RandomForestClassifier(random_state=42)

elif best_model_name == 'XGBoost':
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 6, 9],
        'learning_rate': [0.01, 0.1, 0.2],
        'subsample': [0.8, 0.9, 1.0]
    }
    model = XGBClassifier(random_state=42, eval_metric='logloss')

elif best_model_name == 'Logistic Regression':
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear', 'saga']
    }
    model = LogisticRegression(random_state=42, max_iter=1000)

else:
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'gamma': ['scale', 'auto', 0.1, 0.01],
        'kernel': ['rbf', 'linear']
    }
    model = SVC(random_state=42, probability=True)

# Perform grid search with recall as scoring metric
grid_search = GridSearchCV(
    model, param_grid, cv=5, scoring='recall',
    n_jobs=-1, verbose=1
)

if best_model_name in ['Logistic Regression', 'SVM']:
    grid_search.fit(X_train_scaled, y_train)
else:
    grid_search.fit(X_train, y_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation recall: {grid_search.best_score_:.4f}")

# Train final model with best parameters
best_model = grid_search.best_estimator_

# Save the best model and scaler
joblib.dump(best_model, 'best_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
print("Best model and scaler saved as 'best_model.pkl' and 'scaler.pkl'")

if best_model_name in ['Logistic Regression', 'SVM']:
    y_pred_final = best_model.predict(X_test_scaled)
    y_pred_proba_final = best_model.predict_proba(X_test_scaled)[:, 1]
else:
    y_pred_final = best_model.predict(X_test)
    y_pred_proba_final = best_model.predict_proba(X_test)[:, 1]

# Evaluate final tuned model
final_accuracy = accuracy_score(y_test, y_pred_final)
final_precision = precision_score(y_test, y_pred_final)
final_recall = recall_score(y_test, y_pred_final)
final_f1 = f1_score(y_test, y_pred_final)
final_auc = roc_auc_score(y_test, y_pred_proba_final)

print(f"\nFinal Tuned Model Performance:")
print(f"Accuracy: {final_accuracy:.4f}")
print(f"Precision: {final_precision:.4f}")
print(f"Recall: {final_recall:.4f}")
print(f"F1-Score: {final_f1:.4f}")
print(f"AUC-ROC: {final_auc:.4f}")

# Feature Importance Analysis and Model Interpretation
if hasattr(best_model, 'feature_importances_'):
    plt.figure(figsize=(12, 8))

    feature_importance = pd.DataFrame({
        'feature': X_imputed.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    # Plot top 15 features
    sns.barplot(data=feature_importance.head(15), x='importance', y='feature')
    plt.title(f'Top 15 Most Important Features - {best_model_name}')
    plt.xlabel('Feature Importance')
    plt.tight_layout()
    plt.show()

    print("Top 10 Most Important Features:")
    print(feature_importance.head(10))

elif best_model_name == 'Logistic Regression':
    # For logistic regression, we can look at coefficients
    plt.figure(figsize=(12, 8))

    coefficients = pd.DataFrame({
        'feature': X_imputed.columns,
        'coefficient': best_model.coef_[0]
    }).sort_values('coefficient', key=abs, ascending=False)

    # Plot top 10 coefficients
    sns.barplot(data=coefficients.head(10), x='coefficient', y='feature')
    plt.title(f'Top 15 Most Influential Features - {best_model_name}')
    plt.xlabel('Coefficient Value')
    plt.tight_layout()
    plt.show()

    print("Top 10 Most Influential Features:")
    print(coefficients.head(10))

# deployment-ready prediction

def predict_cervical_cancer_risk(patient_data, model=best_model, scaler=scaler, feature_names=X_imputed.columns):
    """
    Predict cervical cancer risk for a new patient

    Parameters:
    patient_data: dict or array-like containing patient features
    model: trained model
    scaler: fitted scaler
    feature_names: list of feature names

    Returns:
    dict: prediction results
    """

    # Convert to DataFrame if it's a dictionary
    if isinstance(patient_data, dict):
        patient_df = pd.DataFrame([patient_data])
        # Ensure all features are present and in correct order
        for feature in feature_names:
            if feature not in patient_df.columns:
                patient_df[feature] = 0
        patient_df = patient_df[feature_names]
    else:
        patient_df = pd.DataFrame([patient_data], columns=feature_names)

    # Scale the data if needed
    if hasattr(model, 'feature_importances_'):
        prediction = model.predict(patient_df)[0]
        probability = model.predict_proba(patient_df)[0, 1]
    else:
        patient_scaled = scaler.transform(patient_df)
        # Convert scaled array back to DataFrame with feature names to avoid UserWarning
        patient_scaled_df = pd.DataFrame(patient_scaled, columns=feature_names)
        prediction = model.predict(patient_scaled_df)[0]
        probability = model.predict_proba(patient_scaled_df)[:, 1][0]

    risk_level = "High Risk" if prediction == 1 else "Low Risk"
    confidence = probability if prediction == 1 else (1 - probability)

    return {
        'prediction': prediction,
        'probability': probability,
        'risk_level': risk_level,
        'confidence': confidence
    }

# Example usage:
print("Example Prediction:")
sample_patient = {
    'Age': 35,
    'Number of sexual partners': 3,
    'First sexual intercourse': 17,
    'Num of pregnancies': 2,
    'Smokes': 0,
    'Hormonal Contraceptives': 1,
    'IUD': 0,
    'STDs': 0,
    'STDs (number)': 0
}

# Add remaining features with default values
for feature in X_imputed.columns:
    if feature not in sample_patient:
        sample_patient[feature] = 0

prediction_result = predict_cervical_cancer_risk(sample_patient)
print(f"Prediction: {prediction_result}")

import sys

# Install pyngrok if not already installed
!{sys.executable} -m pip install pyngrok

from pyngrok import ngrok
import subprocess
import os

# Replace 'YOUR_NGROK_AUTH_TOKEN' with your actual ngrok token
# You can get one from ngrok.com after signing up
# os.environ["NGROK_AUTHTOKEN"] = "YOUR_NGROK_AUTH_TOKEN"
# ngrok.set_auth_token(os.environ["NGROK_AUTHTOKEN"])

# Start a ngrok tunnel for port 8501 (Streamlit's default port)
# This will download the ngrok executable if it's not already present
try:
    public_url = ngrok.connect(8501)
    print(f"ngrok tunnel URL: {public_url}")

    # In a real scenario, you would now start your Streamlit app.
    # In Colab, you might need to run Streamlit in a separate thread/process.
    # For demonstration, we'll just print a message.
    print("Now, you would typically run your Streamlit app:")
    print("  streamlit run your_app.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false")
    print("Access your Streamlit app at the ngrok tunnel URL above.")

    # To keep the tunnel alive, you might need a loop or keep this cell running
    # ngrok.kill() # Call this when you are done to close the tunnel

except Exception as e:
    print(f"Error starting ngrok tunnel: {e}")
    print("Make sure you have an ngrok auth token set if prompted, or if pyngrok indicates it's needed.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pickle
# import numpy as np
# 
# def main():
#     st.title("ðŸ©º Cervical Cancer Risk Assessment Tool")
#     st.write("A clinical decision support system for healthcare providers")
# 
#     # Patient input form
#     with st.form("patient_form"):
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             age = st.slider("Age", 15, 80, 35)
#             num_partners = st.slider("Number of sexual partners", 1, 30, 3)
#             first_sex = st.slider("Age at first intercourse", 10, 30, 17)
# 
#         with col2:
#             pregnancies = st.slider("Number of pregnancies", 0, 15, 2)
#             smokes = st.selectbox("Smokes", ["No", "Yes"])
#             hormonal_contraceptives = st.selectbox("Uses hormonal contraceptives", ["No", "Yes"])
# 
#         submitted = st.form_submit_button("Assess Risk")
# 
#         if submitted:
#             # Process inputs and make prediction
#             # Load the model and scaler inside the main function to ensure they are available
#             # when the Streamlit app runs in a separate process
#             try:
#                 model = pickle.load(open('best_model.pkl', 'rb'))
#                 scaler = pickle.load(open('scaler.pkl', 'rb'))
#             except FileNotFoundError:
#                 st.error("Error: Model or scaler files not found. Please ensure 'best_model.pkl' and 'scaler.pkl' are in the current directory.")
#                 return
# 
#             # Define feature names based on X_imputed columns used during training
#             # This is a placeholder; in a real app, you'd load these from a config or recreate them
#             feature_names = [
#                 'Age', 'Number of sexual partners', 'First sexual intercourse',
#                 'Num of pregnancies', 'Smokes', 'Smokes (years)', 'Smokes (packs/year)',
#                 'Hormonal Contraceptives', 'Hormonal Contraceptives (years)', 'IUD',
#                 'IUD (years)', 'STDs', 'STDs (number)', 'STDs:condylomatosis',
#                 'STDs:cervical condylomatosis', 'STDs:vaginal condylomatosis',
#                 'STDs:vulvo-perineal condylomatosis', 'STDs:syphilis',
#                 'STDs:pelvic inflammatory disease', 'STDs:genital herpes',
#                 'STDs:molluscum contagiosum', 'STDs:AIDS', 'STDs:HIV',
#                 'STDs:Hepatitis B', 'STDs:HPV', 'STDs: Number of diagnosis',
#                 'Dx:Cancer', 'Dx:CIN', 'Dx:HPV', 'Dx', 'Hinselmann', 'Schiller', 'Citology'
#             ]
# 
#             patient_data = {
#                 'Age': age,
#                 'Number of sexual partners': num_partners,
#                 'First sexual intercourse': first_sex,
#                 'Num of pregnancies': pregnancies,
#                 'Smokes': 1 if smokes == "Yes" else 0,
#                 'Hormonal Contraceptives': 1 if hormonal_contraceptives == "Yes" else 0
#             }
# 
#             # Fill in missing features with default values (e.g., 0 or mean/median from training)
#             # This step is crucial to match the input shape of the trained model.
#             # For a robust app, these defaults should come from the training data statistics.
#             full_patient_data = {}
#             for feature in feature_names:
#                 full_patient_data[feature] = patient_data.get(feature, 0) # Default to 0 if not provided
# 
#             patient_df = pd.DataFrame([full_patient_data])
#             patient_df = patient_df[feature_names] # Ensure correct order of columns
# 
#             # Scale the data
#             patient_scaled = scaler.transform(patient_df)
# 
#             # Make prediction
#             prediction = model.predict(patient_scaled)[0]
#             probability = model.predict_proba(patient_scaled)[0, 1]
# 
#             risk_score = prediction
#             confidence = probability if prediction == 1 else (1 - probability)
# 
#             # Display results
#             if risk_score == 1:
#                 st.error(f"ðŸš¨ HIGH RISK: {confidence:.1%} probability")
#                 st.write("**Recommendation**: Schedule biopsy within 2 weeks")
#             else:
#                 st.success(f"âœ… LOW RISK: {(1-confidence):.1%} confidence")
#                 st.write("**Recommendation**: Routine screening schedule")
# 
# 
# if __name__ == "__main__":
#     main()